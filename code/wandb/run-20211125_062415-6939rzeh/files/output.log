idx |   n |     params |          module |            arguments |   in_channel |   out_channel
----------------------------------------------------------------------------------------------
  0 |   1 |     14,304 |            Conv |      [96, 7, 2, [0]] |            3           96
  1 |   1 |          0 |         MaxPool |         [3, 2, True] |           96           96
  2 |   1 |     11,920 |            Fire |         [16, 64, 64] |           96          128
  3 |   1 |     12,432 |            Fire |         [16, 64, 64] |          128          128
  4 |   1 |     45,344 |            Fire |       [32, 128, 128] |          128          256
  5 |   1 |          0 |         MaxPool |         [3, 2, True] |          256          256
  6 |   1 |     49,440 |            Fire |       [32, 128, 128] |          256          256
  7 |   1 |    104,880 |            Fire |       [48, 192, 192] |          256          384
  8 |   1 |    111,024 |            Fire |       [48, 192, 192] |          384          384
  9 |   1 |    188,992 |            Fire |       [64, 256, 256] |          384          512
 10 |   1 |          0 |         MaxPool |         [3, 2, True] |          512          512
 11 |   1 |    197,184 |            Fire |       [64, 256, 256] |          512          512
 12 |   1 |          0 |         Dropout |                [0.5] |          512          512
 13 |   1 |    514,000 |       FixedConv |         [1000, 1, 1] |          512         1000
 14 |   1 |          0 |   GlobalAvgPool |                   [] |         1000         1000
Model Summary: 70 layers, 1,249,520 parameters, 1,249,520 gradients
Model save path: exp/latest/best.pt
  0%|                                                                                                              | 0/244 [00:00<?, ?it/s]/opt/conda/envs/lightweight/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)



































Train: [001] Loss: 5.357, Acc: 27.96% F1(macro): 0.12: 100%|█████████████████████████████████████████████| 244/244 [01:39<00:00,  2.45it/s]


















 Val:       Loss: 3.010, Acc: 30.86% F1(macro): 0.08: 100%|████████████████████████████████████████████████| 82/82 [00:38<00:00,  2.15it/s]
Model saved. Current best test f1: 0.079









































Train: [002] Loss: 2.367, Acc: 31.08% F1(macro): 0.14: 100%|█████████████████████████████████████████████| 244/244 [01:45<00:00,  2.32it/s]
















 Val:       Loss: 1.901, Acc: 31.76% F1(macro): 0.08: : 95it [00:32,  2.71it/s]
 Val:       Loss: 1.930, Acc: 31.20% F1(macro): 0.08: 100%|████████████████████████████████████████████████| 82/82 [00:34<00:00,  2.38it/s]




Train: [003] Loss: 1.969, Acc: 30.89% F1(macro): 0.12:   9%|████▏                                         | 22/244 [00:12<02:10,  1.70it/s]
Traceback (most recent call last):
  File "train.py", line 134, in <module>
    device=device,
  File "train.py", line 89, in train
    val_dataloader=val_dl if val_dl else test_dl,
  File "/opt/ml/code/src/trainer.py", line 137, in train
    for batch, (data, labels) in pbar:
  File "/opt/conda/envs/lightweight/lib/python3.7/site-packages/tqdm/std.py", line 1180, in __iter__
    for obj in iterable:
  File "/opt/conda/envs/lightweight/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 363, in __next__
    data = self._next_data()
  File "/opt/conda/envs/lightweight/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 974, in _next_data
    idx, data = self._get_data()
  File "/opt/conda/envs/lightweight/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 931, in _get_data
    success, data = self._try_get_data()
  File "/opt/conda/envs/lightweight/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 779, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/opt/conda/envs/lightweight/lib/python3.7/queue.py", line 179, in get
    self.not_empty.wait(remaining)
  File "/opt/conda/envs/lightweight/lib/python3.7/threading.py", line 300, in wait
    gotit = waiter.acquire(True, timeout)
