idx |   n |     params |          module |            arguments |   in_channel |   out_channel
----------------------------------------------------------------------------------------------
  0 |   1 |     14,304 |            Conv |      [96, 7, 2, [0]] |            3           96
  1 |   1 |          0 |         MaxPool |               [3, 2] |           96           96
  2 |   1 |     11,920 |            Fire |         [16, 64, 64] |           96          128
  3 |   1 |     12,432 |            Fire |         [16, 64, 64] |          128          128
  4 |   1 |     45,344 |            Fire |       [32, 128, 128] |          128          256
  5 |   1 |          0 |         MaxPool |               [3, 2] |          256          256
  6 |   1 |     49,440 |            Fire |       [32, 128, 128] |          256          256
  7 |   1 |    104,880 |            Fire |       [48, 192, 192] |          256          384
  8 |   1 |    111,024 |            Fire |       [48, 192, 192] |          384          384
  9 |   1 |    188,992 |            Fire |       [64, 256, 256] |          384          512
 10 |   1 |          0 |         MaxPool |               [3, 2] |          512          512
 11 |   1 |    197,184 |            Fire |       [64, 256, 256] |          512          512
 12 |   1 |          0 |         Dropout |                [0.5] |          512          512
 13 |   1 |    514,000 |       FixedConv |         [1000, 1, 1] |          512         1000
 14 |   1 |          0 |   GlobalAvgPool |                   [] |         1000         1000
Model Summary: 70 layers, 1,249,520 parameters, 1,249,520 gradients
Model save path: exp/latest/best.pt
  0%|                                                                                                              | 0/244 [00:00<?, ?it/s]/opt/conda/envs/lightweight/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)

























Train: [001] Loss: 6.787, Acc: 15.92% F1(macro): 0.10: 100%|█████████████████████████████████████████████| 244/244 [00:54<00:00,  4.45it/s]













 Val:       Loss: 6.529, Acc: 31.41% F1(macro): 0.08: : 94it [00:26,  2.51it/s]
 Val:       Loss: 6.537, Acc: 30.86% F1(macro): 0.08: 100%|████████████████████████████████████████████████| 82/82 [00:28<00:00,  2.91it/s]



























Train: [002] Loss: 6.030, Acc: 31.31% F1(macro): 0.13: 100%|█████████████████████████████████████████████| 244/244 [00:54<00:00,  4.51it/s]












